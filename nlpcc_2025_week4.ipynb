{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYrqzlVoFM/NnzMMs06SzD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natanaelwgm/2025w-PromedUI-NLPCC-Ganjil20242025/blob/main/nlpcc_2025_week4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Install the OpenAI Python library (quietly)\n",
        "!pip install openai -q"
      ],
      "metadata": {
        "id": "l8hA4urdwU9v"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpCn7qu5wEN9",
        "outputId": "7e7cb78f-72c0-4a1e-c637-2a754cf36315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OpenAI API Key loaded successfully from Colab secrets.\n",
            "\n",
            "üöÄ Sending request to OpenAI API with model: gpt-4o and input: 'Hi can you tell me about Universitas Indonesia'...\n",
            "\n",
            "‚úÖ API call successful!\n",
            "\n",
            "üí¨ GPT-4o says:\n",
            "Universitas Indonesia (UI) is one of the most prestigious and oldest educational institutions in Indonesia. Located in Depok, just south of Jakarta, it serves as a leading center for research and higher education in the country. Here are some key points about UI:\n",
            "\n",
            "1. **History and Establishment**: UI traces its origins to 1849, making it one of Indonesia's oldest universities. Its modern form was established in 1950.\n",
            "\n",
            "2. **Campuses**: The university has two main campuses‚Äîone in Salemba, Central Jakarta, and the primary one in Depok. The Depok campus is known for its lush green environment and expansive facilities.\n",
            "\n",
            "3. **Faculties and Programs**: Universitas Indonesia offers a wide range of academic programs through its faculties, including humanities, social sciences, engineering, medicine, law, economics, natural sciences, and more.\n",
            "\n",
            "4. **Research and Innovation**: UI is a hub for research and innovation, contributing significant academic and scientific advancements both nationally and internationally.\n",
            "\n",
            "5. **International Collaboration**: The university actively engages in international collaborations, offering exchange programs and partnerships with institutions worldwide.\n",
            "\n",
            "6. **Recognition**: It's consistently ranked among the top universities in Indonesia and holds a strong position in global university rankings.\n",
            "\n",
            "7. **Student Life**: UI boasts a vibrant student life with numerous organizations, cultural activities, and sports facilities.\n",
            "\n",
            "8. **Alumni**: The university's alumni network includes influential figures in various sectors, including government, business, and academia.\n",
            "\n",
            "Overall, Universitas Indonesia plays a crucial role in shaping Indonesia's intellectual and cultural landscape.\n"
          ]
        }
      ],
      "source": [
        "# @title Say Hello to GPT-4o via OpenAI API\n",
        "#\n",
        "# This single code block will:\n",
        "# 1. Install the OpenAI Python library.\n",
        "# 2. Ask for your OpenAI API key (safely, using Colab secrets if available).\n",
        "# 3. Send a \"Hello\" message to the GPT-4o model using the Responses API.\n",
        "# 4. Print the model's response.\n",
        "\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import openai\n",
        "import os\n",
        "from google.colab import userdata # For Colab secrets\n",
        "import getpass # For securely getting password if secret is not set\n",
        "\n",
        "# Step 3: Get OpenAI API Key\n",
        "# Try to load the API key from Colab secrets first\n",
        "try:\n",
        "    api_key = \"APIKEY\"\n",
        "    if not api_key:\n",
        "        # This will be caught by the KeyError or proceed to the manual input\n",
        "        raise KeyError(\"OPENAI_API_KEY not found or is empty in Colab secrets.\")\n",
        "    print(\"‚úÖ OpenAI API Key loaded successfully from Colab secrets.\")\n",
        "except KeyError:\n",
        "    print(\"‚ö†Ô∏è OpenAI API Key not found in Colab secrets.\")\n",
        "    print(\"   You can add it by clicking the 'üîë' (key) icon in the left sidebar,\")\n",
        "    print(\"   then 'Add new secret' with the name 'OPENAI_API_KEY'.\")\n",
        "    print(\"\\nAlternatively, please paste your OpenAI API key here (less secure):\")\n",
        "    api_key = getpass.getpass('Enter your OpenAI API key: ')\n",
        "    if api_key:\n",
        "        print(\"‚úÖ OpenAI API Key received.\")\n",
        "    else:\n",
        "        print(\"‚ùå No API key provided. Please provide an API key to proceed.\")\n",
        "        # You might want to raise an error or exit here if no key is provided\n",
        "        # For this example, we'll let it proceed and potentially fail at the API call\n",
        "\n",
        "# Step 4: Set the API key for the OpenAI library\n",
        "# The OpenAI client will automatically pick it up if it's set as an environment variable\n",
        "# or you can pass it directly: client = openai.OpenAI(api_key=api_key)\n",
        "if api_key:\n",
        "    os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "    # Step 5: Initialize the OpenAI client and make the API call\n",
        "    try:\n",
        "        client = openai.OpenAI() # API key is read from environment variable\n",
        "\n",
        "        model_id = \"gpt-4o\"\n",
        "        user_input = \"Hi can you tell me about Universitas Indonesia\"\n",
        "\n",
        "        print(f\"\\nüöÄ Sending request to OpenAI API with model: {model_id} and input: '{user_input}'...\")\n",
        "\n",
        "        # Using the \"Responses\" API as recommended for new projects\n",
        "        # POST https://api.openai.com/v1/responses\n",
        "        response = client.responses.create(\n",
        "            model=model_id,\n",
        "            input=user_input\n",
        "        )\n",
        "\n",
        "        print(\"\\n‚úÖ API call successful!\")\n",
        "\n",
        "        # Print the full response object (for debugging or more details)\n",
        "        # print(\"\\nüîç Full API Response Object:\")\n",
        "        # print(response)\n",
        "\n",
        "        # Extract and print the text content from the response\n",
        "        # The Responses API has a convenience property `output_text` in the SDK\n",
        "        # or you can parse it from response.output[0].content[0].text\n",
        "        assistant_reply = \"\"\n",
        "        if hasattr(response, 'output_text') and response.output_text:\n",
        "            assistant_reply = response.output_text\n",
        "        elif response.output and len(response.output) > 0:\n",
        "            first_output_item = response.output[0]\n",
        "            if first_output_item.type == \"message\" and hasattr(first_output_item, 'content') and \\\n",
        "               first_output_item.content and len(first_output_item.content) > 0:\n",
        "                first_content_part = first_output_item.content[0]\n",
        "                if first_content_part.type == \"output_text\" and hasattr(first_content_part, 'text'):\n",
        "                    assistant_reply = first_content_part.text\n",
        "\n",
        "        if assistant_reply:\n",
        "            print(f\"\\nüí¨ GPT-4o says:\")\n",
        "            print(assistant_reply)\n",
        "        else:\n",
        "            print(\"\\n‚ö†Ô∏è Could not extract a text reply from the response. Full response printed above for inspection.\")\n",
        "            print(\"\\nüîç Full API Response Object:\") # Print full response if text extraction fails\n",
        "            print(response)\n",
        "\n",
        "\n",
        "    except openai.APIConnectionError as e:\n",
        "        print(\"‚ùå API Connection Error: The server could not be reached.\")\n",
        "        print(f\"   Error details: {e.__cause__}\")\n",
        "    except openai.RateLimitError as e:\n",
        "        print(\"‚ùå Rate Limit Error: You have exceeded your API quota or rate limit.\")\n",
        "        print(f\"   Error details: {e}\")\n",
        "    except openai.AuthenticationError as e:\n",
        "        print(\"‚ùå Authentication Error: Your API key is incorrect or invalid.\")\n",
        "        print(f\"   Error details: {e}\")\n",
        "    except openai.APIStatusError as e:\n",
        "        print(f\"‚ùå OpenAI API returned an API Error (Status {e.status_code}):\")\n",
        "        print(f\"   Error details: {e.message}\")\n",
        "        # print(f\"   Full response: {e.response}\") # Can be very verbose\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An unexpected error occurred: {e}\")\n",
        "        print(\"   Make sure your API key is correctly set and has permissions for gpt-4o.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ùå API call cannot proceed without an API key.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Block 1: Process Excel and Generate Embeddings\n",
        "#\n",
        "# This block will:\n",
        "# 1. Install pandas, openpyxl, and openai.\n",
        "# 2. Get your OpenAI API key.\n",
        "# 3. Prompt you to upload an Excel file.\n",
        "# 4. Read the 'text' column from the Excel file.\n",
        "# 5. Generate embeddings for each text.\n",
        "# 6. Store texts and their embeddings.\n",
        "\n",
        "# Step 1: Install libraries\n",
        "!pip install pandas openpyxl openai scikit-learn -q"
      ],
      "metadata": {
        "id": "eZefSWoTyKGR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import openai\n",
        "import os\n",
        "from google.colab import userdata, files\n",
        "import getpass\n",
        "import pandas as pd\n",
        "import time # To add slight delays and avoid hitting rate limits too quickly if processing many rows\n",
        "\n",
        "print(\"Libraries installed and imported.\")\n",
        "\n",
        "# Step 3: Get OpenAI API Key\n",
        "try:\n",
        "    api_key = \"apikey\"\n",
        "    if not api_key:\n",
        "        raise KeyError(\"OPENAI_API_KEY not found or is empty in Colab secrets.\")\n",
        "    print(\"‚úÖ OpenAI API Key loaded successfully from Colab secrets.\")\n",
        "except KeyError:\n",
        "    print(\"‚ö†Ô∏è OpenAI API Key not found in Colab secrets.\")\n",
        "    api_key = getpass.getpass('Enter your OpenAI API key: ')\n",
        "    if api_key:\n",
        "        print(\"‚úÖ OpenAI API Key received.\")\n",
        "    else:\n",
        "        print(\"‚ùå No API key provided. Exiting.\")\n",
        "        # Exit if no API key\n",
        "        import sys\n",
        "        sys.exit()\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "client = openai.OpenAI()\n",
        "print(\"OpenAI client initialized.\")\n",
        "\n",
        "# Step 4: Upload Excel file\n",
        "print(\"\\nPlease upload your Excel file ('texts_to_embed.xlsx' with a 'text' column):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"‚ùå No file uploaded. Please run the cell again and upload a file.\")\n",
        "else:\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    print(f\"\\n‚úÖ File '{filename}' uploaded successfully.\")\n",
        "\n",
        "    # Step 5: Read the Excel file\n",
        "    try:\n",
        "        df = pd.read_excel(filename)\n",
        "        if 'text' not in df.columns:\n",
        "            print(f\"‚ùå Error: The Excel file '{filename}' must contain a column named 'text'.\")\n",
        "            print(f\"   Found columns: {df.columns.tolist()}\")\n",
        "        else:\n",
        "            texts_to_embed = df['text'].dropna().astype(str).tolist()\n",
        "            print(f\"‚úÖ Found {len(texts_to_embed)} texts in the 'text' column.\")\n",
        "\n",
        "            # Step 6: Generate embeddings and store them\n",
        "            texts_with_embeddings = [] # This will store {'text': original_text, 'embedding': vector}\n",
        "\n",
        "            embedding_model = \"text-embedding-3-small\" # Efficient and good for most cases\n",
        "            # embedding_model = \"text-embedding-ada-002\" # Older model\n",
        "\n",
        "            print(f\"\\n‚öôÔ∏è Generating embeddings using model: {embedding_model}\")\n",
        "            for i, text_content in enumerate(texts_to_embed):\n",
        "                if not text_content.strip(): # Skip empty strings\n",
        "                    print(f\"   Skipping empty text at row {i+1}.\")\n",
        "                    continue\n",
        "                try:\n",
        "                    print(f\"   Processing text {i+1}/{len(texts_to_embed)}: \\\"{text_content[:50]}...\\\"\")\n",
        "                    response = client.embeddings.create(\n",
        "                        model=embedding_model,\n",
        "                        input=text_content,\n",
        "                        encoding_format=\"float\" # Get float vectors directly\n",
        "                    )\n",
        "                    embedding = response.data[0].embedding\n",
        "                    texts_with_embeddings.append({\n",
        "                        'text': text_content,\n",
        "                        'embedding': embedding\n",
        "                    })\n",
        "                    # Optional: Add a small delay to be kind to the API for very large files\n",
        "                    # time.sleep(0.1)\n",
        "                except openai.APIError as e:\n",
        "                    print(f\"   ‚ùå OpenAI API Error for text {i+1}: {e}\")\n",
        "                    print(f\"      Skipping this text: \\\"{text_content[:50]}...\\\"\")\n",
        "                except Exception as e:\n",
        "                    print(f\"   ‚ùå An unexpected error occurred for text {i+1}: {e}\")\n",
        "                    print(f\"      Skipping this text: \\\"{text_content[:50]}...\\\"\")\n",
        "\n",
        "\n",
        "            if texts_with_embeddings:\n",
        "                print(f\"\\n‚úÖ Successfully generated and stored embeddings for {len(texts_with_embeddings)} texts.\")\n",
        "                print(\"   You can now run Block 2 to perform a similarity search.\")\n",
        "                # Example: print the first stored item\n",
        "                # print(\"\\n   Example of stored data (first item):\")\n",
        "                # print(f\"   Text: {texts_with_embeddings[0]['text']}\")\n",
        "                # print(f\"   Embedding (first 5 dims): {texts_with_embeddings[0]['embedding'][:5]}\")\n",
        "            else:\n",
        "                print(\"\\n‚ö†Ô∏è No embeddings were generated. Check your Excel file or API errors.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: File '{filename}' not found after upload. This shouldn't happen.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred while processing the Excel file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MqTtOFqoyIVh",
        "outputId": "54afe313-e87a-43c7-e227-231714b136ef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed and imported.\n",
            "‚úÖ OpenAI API Key loaded successfully from Colab secrets.\n",
            "OpenAI client initialized.\n",
            "\n",
            "Please upload your Excel file ('texts_to_embed.xlsx' with a 'text' column):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4786fff0-304d-4cbc-9af7-7d223a3d93c4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4786fff0-304d-4cbc-9af7-7d223a3d93c4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving week4_reviews.xlsx to week4_reviews.xlsx\n",
            "\n",
            "‚úÖ File 'week4_reviews.xlsx' uploaded successfully.\n",
            "‚úÖ Found 100 texts in the 'text' column.\n",
            "\n",
            "‚öôÔ∏è Generating embeddings using model: text-embedding-3-small\n",
            "   Processing text 1/100: \"The packaging feels really cheap and flimsy....\"\n",
            "   Processing text 2/100: \"Getting the product out is a nightmare, the pump i...\"\n",
            "   Processing text 3/100: \"My product leaked everywhere because the lid doesn...\"\n",
            "   Processing text 4/100: \"The compact mirror is tiny and basically useless....\"\n",
            "   Processing text 5/100: \"It's impossible to get the last bit of product out...\"\n",
            "   Processing text 6/100: \"The lid cracked after only a couple of uses....\"\n",
            "   Processing text 7/100: \"Too much product comes out with just a slight sque...\"\n",
            "   Processing text 8/100: \"The closure mechanism is stiff and hard to open qu...\"\n",
            "   Processing text 9/100: \"This packaging is bulkier than it needs to be....\"\n",
            "   Processing text 10/100: \"The design of the packaging is awkward to hold and...\"\n",
            "   Processing text 11/100: \"This product feels really heavy and oily on my ski...\"\n",
            "   Processing text 12/100: \"It leaves a greasy residue after I apply it....\"\n",
            "   Processing text 13/100: \"By midday, my face is an absolute oil slick when I...\"\n",
            "   Processing text 14/100: \"It doesn't sink into my skin at all, just sits on ...\"\n",
            "   Processing text 15/100: \"Makes my t-zone look incredibly shiny and greasy w...\"\n",
            "   Processing text 16/100: \"The texture is quite oily and genuinely uncomforta...\"\n",
            "   Processing text 17/100: \"I feel like this is clogging my pores because of h...\"\n",
            "   Processing text 18/100: \"My foundation just slides right off when I use thi...\"\n",
            "   Processing text 19/100: \"It has a distinctly oily, unpleasant finish....\"\n",
            "   Processing text 20/100: \"Feels like I just rubbed cooking oil on my face....\"\n",
            "   Processing text 21/100: \"Within a very short time, I'm already looking noti...\"\n",
            "   Processing text 22/100: \"This product significantly exacerbates my natural ...\"\n",
            "   Processing text 23/100: \"It was described as lightweight, but it feels incr...\"\n",
            "   Processing text 24/100: \"The texture is simply too greasy for my liking....\"\n",
            "   Processing text 25/100: \"Leaves a slick, persistent oily feeling on the ski...\"\n",
            "   Processing text 26/100: \"I have to blot my face constantly throughout the d...\"\n",
            "   Processing text 27/100: \"Didn't expect it to be this oily; it's quite disap...\"\n",
            "   Processing text 28/100: \"My skin feels suffocated and greasy with this prod...\"\n",
            "   Processing text 29/100: \"It actually seems to cause my skin to produce even...\"\n",
            "   Processing text 30/100: \"It just feels like a heavy layer of oil sitting th...\"\n",
            "   Processing text 31/100: \"Makes my skin look disgustingly greasy....\"\n",
            "   Processing text 32/100: \"The formula feels sticky and undeniably oily....\"\n",
            "   Processing text 33/100: \"It's too heavy and oily for everyday casual wear....\"\n",
            "   Processing text 34/100: \"I wish it absorbed into the skin better instead of...\"\n",
            "   Processing text 35/100: \"It doesn't have a pleasant, non-oily finish at all...\"\n",
            "   Processing text 36/100: \"My skin feels like it can't breathe under this oil...\"\n",
            "   Processing text 37/100: \"Definitely not suitable for anyone with even sligh...\"\n",
            "   Processing text 38/100: \"Creates a shiny, oily appearance almost instantly ...\"\n",
            "   Processing text 39/100: \"The feel of this product is just greasy and bother...\"\n",
            "   Processing text 40/100: \"Leaves my skin feeling overly saturated and quite ...\"\n",
            "   Processing text 41/100: \"Love the pigmentation! So intense and beautiful....\"\n",
            "   Processing text 42/100: \"This foundation gives me the most smooth, natural-...\"\n",
            "   Processing text 43/100: \"It stays put all day long, even through heat and h...\"\n",
            "   Processing text 44/100: \"So incredibly easy to blend, applies like an absol...\"\n",
            "   Processing text 45/100: \"The color range is amazing, I finally found my per...\"\n",
            "   Processing text 46/100: \"Feels incredibly lightweight on my skin, like I'm ...\"\n",
            "   Processing text 47/100: \"Gives my skin the most healthy, radiant glow....\"\n",
            "   Processing text 48/100: \"The packaging is sleek, sturdy, and feels really h...\"\n",
            "   Processing text 49/100: \"No strong scent, which is perfect for my sensitive...\"\n",
            "   Processing text 50/100: \"Definitely worth every penny, it performs wonderfu...\"\n",
            "   Processing text 51/100: \"This product is a total game changer for my makeup...\"\n",
            "   Processing text 52/100: \"Blends completely seamlessly, zero patchiness what...\"\n",
            "   Processing text 53/100: \"My pores look noticeably blurred and minimized....\"\n",
            "   Processing text 54/100: \"Lasts through a full 10-hour workday without needi...\"\n",
            "   Processing text 55/100: \"The texture is incredibly smooth and feels luxurio...\"\n",
            "   Processing text 56/100: \"Provides perfect coverage without looking heavy or...\"\n",
            "   Processing text 57/100: \"Adds a beautiful, natural pop of color to my cheek...\"\n",
            "   Processing text 58/100: \"Absolute holy grail status unlocked with this prod...\"\n",
            "   Processing text 59/100: \"Makes my lips feel so hydrated and the color is ju...\"\n",
            "   Processing text 60/100: \"Easy to apply with precision, even for someone not...\"\n",
            "   Processing text 61/100: \"I get so many compliments every single time I wear...\"\n",
            "   Processing text 62/100: \"The formula is incredibly gentle and non-irritatin...\"\n",
            "   Processing text 63/100: \"Gives a lovely, soft matte finish without feeling ...\"\n",
            "   Processing text 64/100: \"Perfect for creating both natural daytime looks an...\"\n",
            "   Processing text 65/100: \"This is genuinely the best eyeliner I have ever us...\"\n",
            "   Processing text 66/100: \"Doesn't crease or fade on my eyelids throughout th...\"\n",
            "   Processing text 67/100: \"My skin looks absolutely flawless when I use this....\"\n",
            "   Processing text 68/100: \"Would highly recommend this product to everyone I ...\"\n",
            "   Processing text 69/100: \"Such a beautiful and inclusive shade range....\"\n",
            "   Processing text 70/100: \"Feels completely comfortable on the skin, you forg...\"\n",
            "   Processing text 71/100: \"The shimmer is perfect, just radiant, not chunky g...\"\n",
            "   Processing text 72/100: \"Doesn't settle into my fine lines or wrinkles....\"\n",
            "   Processing text 73/100: \"A tiny amount goes a very long way for intense col...\"\n",
            "   Processing text 74/100: \"Love the lovely dewy finish this gives my complexi...\"\n",
            "   Processing text 75/100: \"Great value for money considering how well it perf...\"\n",
            "   Processing text 76/100: \"The applicator it comes with is actually really go...\"\n",
            "   Processing text 77/100: \"Builds up beautifully if you want more coverage....\"\n",
            "   Processing text 78/100: \"Doesn't cause any breakouts or congestion on my sk...\"\n",
            "   Processing text 79/100: \"Finally found the perfect everyday nude shade!...\"\n",
            "   Processing text 80/100: \"So easy to remove at the end of the day without sc...\"\n",
            "   Processing text 81/100: \"Has a very pleasant, subtle smell, not overpowerin...\"\n",
            "   Processing text 82/100: \"Gives my lashes incredible length and volume....\"\n",
            "   Processing text 83/100: \"Instantly brightens my under-eyes and looks so smo...\"\n",
            "   Processing text 84/100: \"The packaging is cute, compact, and very practical...\"\n",
            "   Processing text 85/100: \"My skin looks healthier, more even-toned and perfe...\"\n",
            "   Processing text 86/100: \"This particular blush shade is so universally flat...\"\n",
            "   Processing text 87/100: \"Melts into the skin effortlessly and looks so natu...\"\n",
            "   Processing text 88/100: \"This has become an absolute staple in my makeup ba...\"\n",
            "   Processing text 89/100: \"Provides added SPF which is a huge bonus for sun p...\"\n",
            "   Processing text 90/100: \"Doesn't transfer easily onto clothes or masks....\"\n",
            "   Processing text 91/100: \"Leaves a beautiful, natural-looking stained effect...\"\n",
            "   Processing text 92/100: \"Easily the most blendable eyeshadow formula I own....\"\n",
            "   Processing text 93/100: \"Makes my skin feel soft, smooth, and hydrated....\"\n",
            "   Processing text 94/100: \"Doesn't emphasize any texture on my skin....\"\n",
            "   Processing text 95/100: \"Gives a perfect sun-kissed glow without looking or...\"\n",
            "   Processing text 96/100: \"The brush it came with is actually really high qua...\"\n",
            "   Processing text 97/100: \"My makeup looks professionally applied when I use ...\"\n",
            "   Processing text 98/100: \"Perfect for my sensitive skin, absolutely no irrit...\"\n",
            "   Processing text 99/100: \"The color payoff is absolutely insane, in the best...\"\n",
            "   Processing text 100/100: \"Simply an amazing product, it far exceeds all my e...\"\n",
            "\n",
            "‚úÖ Successfully generated and stored embeddings for 100 texts.\n",
            "   You can now run Block 2 to perform a similarity search.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Block 2: Query and Find Top 5 Similar Texts\n",
        "#\n",
        "# This block will:\n",
        "# 1. Take a query text as input.\n",
        "# 2. Generate an embedding for the query.\n",
        "# 3. Calculate cosine similarity against the stored embeddings.\n",
        "# 4. Display the top 5 most similar texts.\n",
        "#\n",
        "# Make sure you have run Block 1 successfully first!\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Check if texts_with_embeddings exists from Block 1\n",
        "if 'texts_with_embeddings' not in globals() or not texts_with_embeddings:\n",
        "    print(\"‚ùå Stored embeddings not found. Please run Block 1 successfully first.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Found {len(texts_with_embeddings)} stored texts with embeddings from Block 1.\")\n",
        "\n",
        "    query_text = input(\"\\nEnter your search query: \")\n",
        "\n",
        "    if not query_text.strip():\n",
        "        print(\"‚ùå Query cannot be empty.\")\n",
        "    else:\n",
        "        try:\n",
        "            # Step 1: Generate embedding for the query\n",
        "            print(f\"\\n‚öôÔ∏è Generating embedding for query: \\\"{query_text}\\\"\")\n",
        "            embedding_model = \"text-embedding-3-small\" # Must match the model used in Block 1\n",
        "            response = client.embeddings.create(\n",
        "                model=embedding_model,\n",
        "                input=query_text,\n",
        "                encoding_format=\"float\"\n",
        "            )\n",
        "            query_embedding = response.data[0].embedding\n",
        "            print(\"‚úÖ Query embedding generated.\")\n",
        "\n",
        "            # Step 2: Calculate cosine similarities\n",
        "            # Reshape query_embedding to be a 2D array for scikit-learn's cosine_similarity\n",
        "            query_embedding_reshaped = np.array(query_embedding).reshape(1, -1)\n",
        "\n",
        "            similarities = []\n",
        "            for item in texts_with_embeddings:\n",
        "                stored_embedding_reshaped = np.array(item['embedding']).reshape(1, -1)\n",
        "                # cosine_similarity returns a 2D array, e.g., [[0.85]]\n",
        "                sim_score = cosine_similarity(query_embedding_reshaped, stored_embedding_reshaped)[0][0]\n",
        "                similarities.append({\n",
        "                    'text': item['text'],\n",
        "                    'score': sim_score\n",
        "                })\n",
        "\n",
        "            # Step 3: Sort by similarity score in descending order\n",
        "            similarities_sorted = sorted(similarities, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "            # Step 4: Display top 5 results\n",
        "            print(f\"\\nüèÜ Top 5 most similar texts to \\\"{query_text}\\\":\")\n",
        "            if not similarities_sorted:\n",
        "                print(\"   No similar texts found (this is unlikely unless stored embeddings are empty).\")\n",
        "            else:\n",
        "                for i, sim_item in enumerate(similarities_sorted[:5]):\n",
        "                    print(f\"   {i+1}. Score: {sim_item['score']:.4f} - Text: \\\"{sim_item['text']}\\\"\")\n",
        "\n",
        "        except openai.APIError as e:\n",
        "            print(f\"‚ùå OpenAI API Error during query processing: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå An unexpected error occurred during query processing: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSnLUty3yb7Y",
        "outputId": "dd805177-319d-4df1-f9fa-4c91e2f9e95f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Found 100 stored texts with embeddings from Block 1.\n",
            "\n",
            "Enter your search query: berminyak\n",
            "\n",
            "‚öôÔ∏è Generating embedding for query: \"berminyak\"\n",
            "‚úÖ Query embedding generated.\n",
            "\n",
            "üèÜ Top 5 most similar texts to \"berminyak\":\n",
            "   1. Score: 0.2964 - Text: \"Leaves a slick, persistent oily feeling on the skin's surface.\"\n",
            "   2. Score: 0.2822 - Text: \"Didn't expect it to be this oily; it's quite disappointing.\"\n",
            "   3. Score: 0.2755 - Text: \"It has a distinctly oily, unpleasant finish.\"\n",
            "   4. Score: 0.2740 - Text: \"Creates a shiny, oily appearance almost instantly upon application.\"\n",
            "   5. Score: 0.2615 - Text: \"Feels like I just rubbed cooking oil on my face.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import openai\n",
        "import os\n",
        "from google.colab import userdata, files\n",
        "import getpass\n",
        "import pandas as pd\n",
        "import time # To add slight delays and avoid hitting rate limits too quickly if processing many rows\n",
        "\n",
        "print(\"Libraries installed and imported.\")\n",
        "\n",
        "# Step 3: Get OpenAI API Key\n",
        "try:\n",
        "    api_key = \"Apikey\"\n",
        "    if not api_key:\n",
        "        raise KeyError(\"OPENAI_API_KEY not found or is empty in Colab secrets.\")\n",
        "    print(\"‚úÖ OpenAI API Key loaded successfully from Colab secrets.\")\n",
        "except KeyError:\n",
        "    print(\"‚ö†Ô∏è OpenAI API Key not found in Colab secrets.\")\n",
        "    api_key = getpass.getpass('Enter your OpenAI API key: ')\n",
        "    if api_key:\n",
        "        print(\"‚úÖ OpenAI API Key received.\")\n",
        "    else:\n",
        "        print(\"‚ùå No API key provided. Exiting.\")\n",
        "        # Exit if no API key\n",
        "        import sys\n",
        "        sys.exit()\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "client = openai.OpenAI()\n",
        "print(\"OpenAI client initialized.\")\n",
        "\n",
        "# Step 4: Upload Excel file\n",
        "print(\"\\nPlease upload your Excel file ('texts_to_embed.xlsx' with a 'text' column):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"‚ùå No file uploaded. Please run the cell again and upload a file.\")\n",
        "else:\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    print(f\"\\n‚úÖ File '{filename}' uploaded successfully.\")\n",
        "\n",
        "    # Step 5: Read the Excel file\n",
        "    try:\n",
        "        df = pd.read_excel(filename)\n",
        "        if 'text' not in df.columns:\n",
        "            print(f\"‚ùå Error: The Excel file '{filename}' must contain a column named 'text'.\")\n",
        "            print(f\"   Found columns: {df.columns.tolist()}\")\n",
        "        else:\n",
        "            texts_to_embed = df['text'].dropna().astype(str).tolist()\n",
        "            print(f\"‚úÖ Found {len(texts_to_embed)} texts in the 'text' column.\")\n",
        "\n",
        "            # Step 6: Generate embeddings and store them\n",
        "            texts_with_embeddings = [] # This will store {'text': original_text, 'embedding': vector}\n",
        "\n",
        "            embedding_model = \"text-embedding-3-small\" # Efficient and good for most cases\n",
        "            # embedding_model = \"text-embedding-ada-002\" # Older model\n",
        "\n",
        "            print(f\"\\n‚öôÔ∏è Generating embeddings using model: {embedding_model}\")\n",
        "            for i, text_content in enumerate(texts_to_embed):\n",
        "                if not text_content.strip(): # Skip empty strings\n",
        "                    print(f\"   Skipping empty text at row {i+1}.\")\n",
        "                    continue\n",
        "                try:\n",
        "                    print(f\"   Processing text {i+1}/{len(texts_to_embed)}: \\\"{text_content[:50]}...\\\"\")\n",
        "                    response = client.embeddings.create(\n",
        "                        model=embedding_model,\n",
        "                        input=text_content,\n",
        "                        encoding_format=\"float\" # Get float vectors directly\n",
        "                    )\n",
        "                    embedding = response.data[0].embedding\n",
        "                    texts_with_embeddings.append({\n",
        "                        'text': text_content,\n",
        "                        'embedding': embedding\n",
        "                    })\n",
        "                    # Optional: Add a small delay to be kind to the API for very large files\n",
        "                    # time.sleep(0.1)\n",
        "                except openai.APIError as e:\n",
        "                    print(f\"   ‚ùå OpenAI API Error for text {i+1}: {e}\")\n",
        "                    print(f\"      Skipping this text: \\\"{text_content[:50]}...\\\"\")\n",
        "                except Exception as e:\n",
        "                    print(f\"   ‚ùå An unexpected error occurred for text {i+1}: {e}\")\n",
        "                    print(f\"      Skipping this text: \\\"{text_content[:50]}...\\\"\")\n",
        "\n",
        "\n",
        "            if texts_with_embeddings:\n",
        "                print(f\"\\n‚úÖ Successfully generated and stored embeddings for {len(texts_with_embeddings)} texts.\")\n",
        "                print(\"   You can now run Block 2 to perform a similarity search.\")\n",
        "                # Example: print the first stored item\n",
        "                # print(\"\\n   Example of stored data (first item):\")\n",
        "                # print(f\"   Text: {texts_with_embeddings[0]['text']}\")\n",
        "                # print(f\"   Embedding (first 5 dims): {texts_with_embeddings[0]['embedding'][:5]}\")\n",
        "            else:\n",
        "                print(\"\\n‚ö†Ô∏è No embeddings were generated. Check your Excel file or API errors.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: File '{filename}' not found after upload. This shouldn't happen.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred while processing the Excel file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "4n6xhIbH20tJ",
        "outputId": "33af2fac-9f85-41b4-9f24-a7ae3eba6038"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed and imported.\n",
            "‚úÖ OpenAI API Key loaded successfully from Colab secrets.\n",
            "OpenAI client initialized.\n",
            "\n",
            "Please upload your Excel file ('texts_to_embed.xlsx' with a 'text' column):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dde901de-72c5-4fba-a1b7-dff4c4b7f59e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dde901de-72c5-4fba-a1b7-dff4c4b7f59e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving week4_news.xlsx to week4_news.xlsx\n",
            "\n",
            "‚úÖ File 'week4_news.xlsx' uploaded successfully.\n",
            "‚úÖ Found 30 texts in the 'text' column.\n",
            "\n",
            "‚öôÔ∏è Generating embeddings using model: text-embedding-3-small\n",
            "   Processing text 1/30: \"Lakers Dominate Pacers in Game 3 Win...\"\n",
            "   Processing text 2/30: \"Rising Star Rookie Breaks Scoring Record in Thrill...\"\n",
            "   Processing text 3/30: \"Veteran Guard Announces Retirement After Champions...\"\n",
            "   Processing text 4/30: \"Team Trades Key Player Ahead of Deadline Buzz...\"\n",
            "   Processing text 5/30: \"Coach Explains Strategic Foul Call in Final Second...\"\n",
            "   Processing text 6/30: \"College Hoops Upset Shocks Nation...\"\n",
            "   Processing text 7/30: \"Injuries Plague Frontcourt as Playoffs Approach...\"\n",
            "   Processing text 8/30: \"NBA Investigating Altercation Between Players...\"\n",
            "   Processing text 9/30: \"New Arena Unveiled with Exhibition Game...\"\n",
            "   Processing text 10/30: \"Fantasy Basketball Draft: Who to Pick Early...\"\n",
            "   Processing text 11/30: \"Local Man Charged in Apparent Robbery-Gone-Wrong H...\"\n",
            "   Processing text 12/30: \"Police Seek Public's Help in Unsolved Cold Case Mu...\"\n",
            "   Processing text 13/30: \"Witness Testimony Rocks High-Profile Murder Trial...\"\n",
            "   Processing text 14/30: \"Investigation Launched After Body Discovered in Pa...\"\n",
            "   Processing text 15/30: \"Family Mourns Victim in Tragic Domestic Incident...\"\n",
            "   Processing text 16/30: \"Arrest Made Hours After Fatal Downtown Shooting...\"\n",
            "   Processing text 17/30: \"Authorities Release Sketch of Suspect in Highway M...\"\n",
            "   Processing text 18/30: \"Jurors Deliberate Fate of Accused Killer...\"\n",
            "   Processing text 19/30: \"Mystery Deepens as Second Victim Found in Area...\"\n",
            "   Processing text 20/30: \"Expert Forensic Analysis Key to Solving Brutal Cri...\"\n",
            "   Processing text 21/30: \"Spring Trends Revealed: What to Wear This Season...\"\n",
            "   Processing text 22/30: \"Sustainable Fashion Takes Center Stage at Design W...\"\n",
            "   Processing text 23/30: \"The Must-Have Skincare Ingredient Experts Are Ravi...\"\n",
            "   Processing text 24/30: \"Celebrity Makeup Artist Shares Red Carpet Secrets...\"\n",
            "   Processing text 25/30: \"Latest Hair Color Ideas to Transform Your Look...\"\n",
            "   Processing text 26/30: \"Affordable Style: Finding Chic Looks on a Budget...\"\n",
            "   Processing text 27/30: \"The Evolution of the Little Black Dress...\"\n",
            "   Processing text 28/30: \"Beauty Hacks for Busy Mornings...\"\n",
            "   Processing text 29/30: \"Iconic Fashion Moments That Defined a Decade...\"\n",
            "   Processing text 30/30: \"Decoding Ingredient Labels: A Guide to Clean Beaut...\"\n",
            "\n",
            "‚úÖ Successfully generated and stored embeddings for 30 texts.\n",
            "   You can now run Block 2 to perform a similarity search.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOK KODE UNTUK VISUALISASI T-SNE (SETELAH BLOK EMBEDDING DARI EXCEL) ---\n",
        "# Pastikan library yang dibutuhkan sudah terinstall jika belum\n",
        "# !pip install scikit-learn plotly pandas numpy # numpy & pandas mungkin sudah dari blok sebelumnya\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.express as px\n",
        "\n",
        "# Memastikan variabel 'texts_with_embeddings' ada dari blok sebelumnya\n",
        "if 'texts_with_embeddings' not in locals() or not texts_with_embeddings:\n",
        "    print(\"‚ùå Error: Variabel 'texts_with_embeddings' tidak ditemukan atau kosong.\")\n",
        "    print(\"   Pastikan blok kode sebelumnya (untuk embedding dari Excel) sudah dijalankan dengan sukses.\")\n",
        "    # Bisa tambahkan sys.exit() jika ingin menghentikan eksekusi di sini\n",
        "    # import sys\n",
        "    # sys.exit(\"Hentikan eksekusi karena data embedding tidak tersedia.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Ditemukan {len(texts_with_embeddings)} item dalam 'texts_with_embeddings'.\")\n",
        "\n",
        "    # Step 1: Ekstrak embeddings dan teks dari 'texts_with_embeddings'\n",
        "    # `texts_with_embeddings` adalah list of dictionaries: [{'text': ..., 'embedding': ...}, ...]\n",
        "\n",
        "    # Inisialisasi list kosong untuk menampung embeddings dan teks\n",
        "    all_embeddings_list = []\n",
        "    all_texts_list = []\n",
        "\n",
        "    for item in texts_with_embeddings:\n",
        "        if 'embedding' in item and 'text' in item:\n",
        "            all_embeddings_list.append(item['embedding'])\n",
        "            all_texts_list.append(item['text'])\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Peringatan: Item dalam 'texts_with_embeddings' tidak memiliki 'embedding' atau 'text': {item}\")\n",
        "\n",
        "    if not all_embeddings_list or not all_texts_list:\n",
        "        print(\"‚ùå Error: Tidak ada embedding atau teks yang valid yang bisa diekstrak.\")\n",
        "        # import sys\n",
        "        # sys.exit(\"Hentikan eksekusi karena tidak ada data valid untuk t-SNE.\")\n",
        "    else:\n",
        "        # Konversi list of embeddings ke NumPy array\n",
        "        embeddings_np = np.array(all_embeddings_list)\n",
        "        # List teks sudah siap (all_texts_list)\n",
        "\n",
        "        print(f\"   Berhasil mengekstrak {embeddings_np.shape[0]} embeddings dengan dimensi {embeddings_np.shape[1]}.\")\n",
        "        print(f\"   Berhasil mengekstrak {len(all_texts_list)} label teks.\")\n",
        "\n",
        "\n",
        "        # Step 2: Reduksi dimensi menggunakan t-SNE dengan pengaturan default\n",
        "        print(\"\\nüîÑ Melakukan reduksi dimensi dengan t-SNE (pengaturan default)...\")\n",
        "\n",
        "        n_samples = embeddings_np.shape[0]\n",
        "\n",
        "        if n_samples <= 1:\n",
        "            print(f\"‚ùå Error: t-SNE membutuhkan setidaknya 2 sampel. Ditemukan: {n_samples}\")\n",
        "        else:\n",
        "            perplexity_value = min(30.0, float(n_samples - 1))\n",
        "            if perplexity_value < 5 and n_samples > 1:\n",
        "                 print(f\"   ‚ö†Ô∏è Jumlah sampel ({n_samples}) kecil. Menggunakan perplexity={perplexity_value:.1f}.\")\n",
        "            if perplexity_value == 0: # Untuk kasus n_samples = 1\n",
        "                perplexity_value = 1.0\n",
        "\n",
        "            tsne_model = TSNE(\n",
        "                n_components=2,\n",
        "                perplexity=perplexity_value,\n",
        "                learning_rate='auto',\n",
        "                init='pca',\n",
        "                n_iter=1000,\n",
        "                random_state=42, # Untuk hasil yang konsisten\n",
        "                verbose=0\n",
        "            )\n",
        "            tsne_results = tsne_model.fit_transform(embeddings_np)\n",
        "            print(f\"‚úÖ Reduksi dimensi t-SNE selesai. Shape hasil: {tsne_results.shape}\")\n",
        "\n",
        "            # Step 3: Buat DataFrame untuk Plotly\n",
        "            df_tsne = pd.DataFrame({\n",
        "                'tsne_x': tsne_results[:, 0],\n",
        "                'tsne_y': tsne_results[:, 1],\n",
        "                'text_label': all_texts_list # Teks asli untuk hover\n",
        "            })\n",
        "\n",
        "            # (Opsional) Jika kamu punya kolom kategori di DataFrame `df` awal,\n",
        "            # kamu bisa coba menambahkannya di sini untuk pewarnaan.\n",
        "            # Misalnya, jika ada kolom 'category' di `df` dan urutannya sama:\n",
        "            # if 'category' in df.columns and len(df['category'].dropna()) == len(all_texts_list):\n",
        "            #     # Ini asumsi bahwa texts_to_embed diambil tanpa dropna yang mengubah urutan signifikan\n",
        "            #     # atau bahwa kita memfilter df asli agar sesuai dengan texts_with_embeddings\n",
        "            #     # Untuk cara yang lebih aman, sebaiknya simpan kategori bersamaan dengan embedding\n",
        "            #     # Namun untuk sekarang, kita coba cara sederhana:\n",
        "            #     try:\n",
        "            #         # Ambil kategori yang sesuai dengan teks yang berhasil di-embed\n",
        "            #         # Ini agak tricky jika ada teks yang di-skip.\n",
        "            #         # Cara paling aman adalah menyimpan kategori saat looping embedding.\n",
        "            #         # Untuk kesederhanaan, kita asumsikan urutan masih terjaga jika tidak ada skip.\n",
        "            #         if len(texts_to_embed) == len(df['text'].dropna()): # Jika tidak ada skip\n",
        "            #             df_tsne['category'] = df[df['text'].isin(all_texts_list)]['category'].tolist() # ini masih berisiko jika ada duplikat teks\n",
        "            #             color_column_name = 'category'\n",
        "            #             print(\"   Menambahkan kolom kategori untuk pewarnaan.\")\n",
        "            #         else:\n",
        "            #             print(\"   ‚ö†Ô∏è Tidak dapat menambahkan kategori secara otomatis karena ada teks yang mungkin di-skip.\")\n",
        "            #             color_column_name = None\n",
        "            #     except Exception as e:\n",
        "            #         print(f\"   ‚ö†Ô∏è Gagal menambahkan kategori: {e}\")\n",
        "            #         color_column_name = None\n",
        "            # else:\n",
        "            #     color_column_name = None\n",
        "\n",
        "            # Untuk saat ini, kita tidak menggunakan pewarnaan kategori otomatis dari df awal\n",
        "            # karena kompleksitas mencocokkan kembali jika ada teks yang di-skip.\n",
        "            # Jika kamu menyimpan kategori di `texts_with_embeddings` (misal `item['category']`),\n",
        "            # maka akan lebih mudah.\n",
        "            color_column_name = None\n",
        "\n",
        "\n",
        "            print(\"\\nüìä Membuat visualisasi t-SNE interaktif dengan Plotly...\")\n",
        "\n",
        "            # Step 4: Visualisasi Interaktif dengan Plotly\n",
        "            fig = px.scatter(\n",
        "                df_tsne,\n",
        "                x='tsne_x',\n",
        "                y='tsne_y',\n",
        "                hover_name='text_label',  # Tampilkan teks dari kolom 'text_label' saat hover\n",
        "                color=color_column_name,  # Akan None jika kategori tidak ditambahkan\n",
        "                title='t-SNE Visualization of Text Embeddings from Excel',\n",
        "                labels={'tsne_x': 't-SNE Component 1', 'tsne_y': 't-SNE Component 2'}\n",
        "            )\n",
        "\n",
        "            fig.update_traces(\n",
        "                marker=dict(size=8, line=dict(width=1, color='DarkSlateGrey')),\n",
        "                selector=dict(mode='markers')\n",
        "            )\n",
        "            fig.update_layout(\n",
        "                hovermode='closest',\n",
        "                legend_title_text='Category' if color_column_name else '',\n",
        "                width=900,\n",
        "                height=700\n",
        "            )\n",
        "\n",
        "            fig.show()\n",
        "\n",
        "            print(\"\\nüéâ Visualisasi selesai! Arahkan kursor ke titik-titik untuk melihat label teksnya.\")\n",
        "            print(\"   Kelompok titik yang berdekatan menunjukkan teks dengan embedding yang serupa.\")\n",
        "\n",
        "# Jika 'texts_with_embeddings' tidak ada (misalnya, blok sebelumnya belum dijalankan)\n",
        "# akan ada pesan error dari blok if/else di atas."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "id": "CSMG-YHE29Ap",
        "outputId": "4511f3d8-dbe4-4c01-e8aa-24163cefaebe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Ditemukan 30 item dalam 'texts_with_embeddings'.\n",
            "   Berhasil mengekstrak 30 embeddings dengan dimensi 1536.\n",
            "   Berhasil mengekstrak 30 label teks.\n",
            "\n",
            "üîÑ Melakukan reduksi dimensi dengan t-SNE (pengaturan default)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Reduksi dimensi t-SNE selesai. Shape hasil: (30, 2)\n",
            "\n",
            "üìä Membuat visualisasi t-SNE interaktif dengan Plotly...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1e2568d3-2dee-4bb4-9489-eac2d3c7499d\" class=\"plotly-graph-div\" style=\"height:700px; width:900px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1e2568d3-2dee-4bb4-9489-eac2d3c7499d\")) {                    Plotly.newPlot(                        \"1e2568d3-2dee-4bb4-9489-eac2d3c7499d\",                        [{\"hovertemplate\":\"\\u003cb\\u003e%{hovertext}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003et-SNE Component 1=%{x}\\u003cbr\\u003et-SNE Component 2=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"hovertext\":[\"Lakers Dominate Pacers in Game 3 Win\",\"Rising Star Rookie Breaks Scoring Record in Thriller\",\"Veteran Guard Announces Retirement After Championship Season\",\"Team Trades Key Player Ahead of Deadline Buzz\",\"Coach Explains Strategic Foul Call in Final Seconds\",\"College Hoops Upset Shocks Nation\",\"Injuries Plague Frontcourt as Playoffs Approach\",\"NBA Investigating Altercation Between Players\",\"New Arena Unveiled with Exhibition Game\",\"Fantasy Basketball Draft: Who to Pick Early\",\"Local Man Charged in Apparent Robbery-Gone-Wrong Homicide\",\"Police Seek Public's Help in Unsolved Cold Case Murder\",\"Witness Testimony Rocks High-Profile Murder Trial\",\"Investigation Launched After Body Discovered in Park\",\"Family Mourns Victim in Tragic Domestic Incident\",\"Arrest Made Hours After Fatal Downtown Shooting\",\"Authorities Release Sketch of Suspect in Highway Murder\",\"Jurors Deliberate Fate of Accused Killer\",\"Mystery Deepens as Second Victim Found in Area\",\"Expert Forensic Analysis Key to Solving Brutal Crime\",\"Spring Trends Revealed: What to Wear This Season\",\"Sustainable Fashion Takes Center Stage at Design Week\",\"The Must-Have Skincare Ingredient Experts Are Raving About\",\"Celebrity Makeup Artist Shares Red Carpet Secrets\",\"Latest Hair Color Ideas to Transform Your Look\",\"Affordable Style: Finding Chic Looks on a Budget\",\"The Evolution of the Little Black Dress\",\"Beauty Hacks for Busy Mornings\",\"Iconic Fashion Moments That Defined a Decade\",\"Decoding Ingredient Labels: A Guide to Clean Beauty\"],\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":1},\"size\":8},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[21.496162,25.489033,-60.54814,-61.675446,90.04263,-9.794589,-44.70873,64.42467,-42.78175,-49.97483,55.898895,96.173615,-5.0682545,-13.029972,-86.22136,57.969303,80.68288,-81.74976,-28.804007,22.540863,-2.8664644,4.488984,27.462706,-29.019838,-6.4941688,-92.32679,56.394215,-93.6204,28.47379,-118.008064],\"xaxis\":\"x\",\"y\":[-34.00547,-2.3119204,-11.0331545,29.785957,57.058872,96.37586,-46.483707,-66.648476,-85.63679,76.19535,96.76775,10.061435,-82.67043,-35.881664,-91.5508,-1.2986668,-31.35753,-47.4474,-3.920138,34.641544,13.055823,132.32118,-68.234985,36.214264,59.891285,53.67307,39.30639,4.8108172,71.56494,-26.28759],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"t-SNE Component 1\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"t-SNE Component 2\"}},\"legend\":{\"tracegroupgap\":0,\"title\":{\"text\":\"\"}},\"title\":{\"text\":\"t-SNE Visualization of Text Embeddings from Excel\"},\"hovermode\":\"closest\",\"width\":900,\"height\":700},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1e2568d3-2dee-4bb4-9489-eac2d3c7499d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéâ Visualisasi selesai! Arahkan kursor ke titik-titik untuk melihat label teksnya.\n",
            "   Kelompok titik yang berdekatan menunjukkan teks dengan embedding yang serupa.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (kode sebelumnya untuk mendapatkan embeddings_np dan all_texts_list) ...\n",
        "\n",
        "n_samples = embeddings_np.shape[0]\n",
        "\n",
        "if n_samples <= 1:\n",
        "    print(f\"‚ùå Error: t-SNE membutuhkan setidaknya 2 sampel. Ditemukan: {n_samples}\")\n",
        "else:\n",
        "    # --- AWAL BAGIAN YANG DIMODIFIKASI UNTUK EKSPERIMEN ---\n",
        "    print(\"\\nüß™ Eksperimen dengan parameter t-SNE:\")\n",
        "\n",
        "    # Pilihan 1: Perplexity lebih rendah\n",
        "    custom_perplexity = 10 # Coba nilai antara 5 dan n_samples-1\n",
        "    if custom_perplexity >= n_samples:\n",
        "        custom_perplexity = max(1.0, float(n_samples - 1) / 2.0) # Pastikan valid\n",
        "\n",
        "    print(f\"   Menggunakan perplexity: {custom_perplexity}\")\n",
        "    print(f\"   Menggunakan n_iter: 2500\") # Tingkatkan iterasi\n",
        "    # print(f\"   Menggunakan early_exaggeration: 15.0\")\n",
        "\n",
        "    tsne_model = TSNE(\n",
        "        n_components=2,\n",
        "        perplexity=custom_perplexity,       # Diubah\n",
        "        learning_rate='auto',           # Biasanya 'auto' atau 200.0 sudah baik\n",
        "        init='pca',\n",
        "        n_iter=2500,                    # Diubah (ditingkatkan)\n",
        "        # early_exaggeration=15.0,      # Opsional: coba ubah ini juga\n",
        "        random_state=42,\n",
        "        verbose=0\n",
        "    )\n",
        "    # --- AKHIR BAGIAN YANG DIMODIFIKASI ---\n",
        "\n",
        "    tsne_results = tsne_model.fit_transform(embeddings_np)\n",
        "    print(f\"‚úÖ Reduksi dimensi t-SNE selesai. Shape hasil: {tsne_results.shape}\")\n",
        "\n",
        "    # ... (sisa kode untuk membuat DataFrame dan plot Plotly tetap sama) ...\n",
        "    df_tsne = pd.DataFrame({\n",
        "        'tsne_x': tsne_results[:, 0],\n",
        "        'tsne_y': tsne_results[:, 1],\n",
        "        'text_label': all_texts_list\n",
        "    })\n",
        "\n",
        "    # (Kode untuk pewarnaan kategori jika ada)\n",
        "    color_column_name = None\n",
        "    # ...\n",
        "\n",
        "    print(\"\\nüìä Membuat visualisasi t-SNE interaktif dengan Plotly...\")\n",
        "    fig = px.scatter(\n",
        "        df_tsne,\n",
        "        x='tsne_x',\n",
        "        y='tsne_y',\n",
        "        hover_name='text_label',\n",
        "        color=color_column_name,\n",
        "        title='t-SNE Visualization of Text Embeddings (Custom Parameters)',\n",
        "        labels={'tsne_x': 't-SNE Component 1', 'tsne_y': 't-SNE Component 2'}\n",
        "    )\n",
        "    fig.update_traces(\n",
        "        marker=dict(size=8, line=dict(width=1, color='DarkSlateGrey')),\n",
        "        selector=dict(mode='markers')\n",
        "    )\n",
        "    fig.update_layout(\n",
        "        hovermode='closest',\n",
        "        legend_title_text='Category' if color_column_name else '',\n",
        "        width=900,\n",
        "        height=700\n",
        "    )\n",
        "    fig.show()\n",
        "    print(\"\\nüéâ Visualisasi selesai!\")"
      ],
      "metadata": {
        "id": "8DZBnnWh3p3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOK 1: EMBEDDING DARI EXCEL ---\n",
        "\n",
        "# Step 1: Install libraries (jika belum)\n",
        "# !pip install openai pandas openpyxl # openpyxl dibutuhkan untuk pd.read_excel\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import openai\n",
        "import os\n",
        "from google.colab import userdata, files # Untuk Colab\n",
        "import getpass # Untuk input API key jika tidak di Colab secrets\n",
        "import pandas as pd\n",
        "import time # Untuk delay opsional\n",
        "import numpy as np # Untuk t-SNE\n",
        "from sklearn.manifold import TSNE # Untuk t-SNE\n",
        "import plotly.express as px # Untuk visualisasi interaktif\n",
        "import sys # Untuk keluar jika ada error fatal\n",
        "\n",
        "print(\"Libraries installed and imported.\")\n",
        "\n",
        "# Step 3: Get OpenAI API Key\n",
        "# Menggunakan API key yang sudah di-hardcode di contoh user,\n",
        "# idealnya menggunakan Colab secrets atau input aman.\n",
        "# api_key_user_provided = \"Apikey\" # CONTOH DARI USER\n",
        "# Gunakan salah satu metode di bawah ini:\n",
        "\n",
        "API_KEY_NAME_IN_SECRETS = 'Apikey' # Ganti jika nama secretmu berbeda\n",
        "\n",
        "try:\n",
        "    # Coba dari Colab secrets (lebih aman)\n",
        "    api_key = \"Apikey\"\n",
        "    if not api_key:\n",
        "        # Jika ada di secrets tapi kosong\n",
        "        print(f\"‚ö†Ô∏è {API_KEY_NAME_IN_SECRETS} ditemukan di Colab secrets tapi kosong.\")\n",
        "        raise KeyError # Jatuh ke input manual\n",
        "    print(f\"‚úÖ OpenAI API Key loaded successfully from Colab secrets ('{API_KEY_NAME_IN_SECRETS}').\")\n",
        "except KeyError:\n",
        "    print(f\"‚ö†Ô∏è OpenAI API Key ('{API_KEY_NAME_IN_SECRETS}') not found in Colab secrets.\")\n",
        "    print(\"   You can add it by clicking the 'üîë' (key) icon in the left sidebar,\")\n",
        "    print(f\"   then 'Add new secret' with the name '{API_KEY_NAME_IN_SECRETS}'.\")\n",
        "    print(\"\\nAlternatively, please paste your OpenAI API key here (less secure):\")\n",
        "    api_key = getpass.getpass('Enter your OpenAI API key: ')\n",
        "    if api_key:\n",
        "        print(\"‚úÖ OpenAI API Key received via manual input.\")\n",
        "    else:\n",
        "        print(\"‚ùå No API key provided. Exiting.\")\n",
        "        sys.exit(\"API Key is required to proceed.\")\n",
        "# except Exception as e: # Untuk menangkap error lain saat akses userdata, jika ada\n",
        "# print(f\"An error occurred trying to access Colab secrets: {e}\")\n",
        "# print(\"Proceeding to manual API key input.\")\n",
        "# api_key = getpass.getpass('Enter your OpenAI API key: ')\n",
        "# if api_key:\n",
        "# print(\"‚úÖ OpenAI API Key received via manual input.\")\n",
        "# else:\n",
        "# print(\"‚ùå No API key provided. Exiting.\")\n",
        "# sys.exit(\"API Key is required to proceed.\")\n",
        "\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "try:\n",
        "    client = openai.OpenAI()\n",
        "    # Coba lakukan panggilan tes sederhana (opsional, tapi bagus untuk validasi awal)\n",
        "    # client.models.list()\n",
        "    print(\"‚úÖ OpenAI client initialized successfully.\")\n",
        "except openai.AuthenticationError:\n",
        "    print(\"‚ùå OpenAI Authentication Error: Your API key is invalid or has been revoked.\")\n",
        "    print(\"   Please check your API key and try again.\")\n",
        "    sys.exit(\"Exiting due to Authentication Error.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to initialize OpenAI client: {e}\")\n",
        "    sys.exit(\"Exiting due to OpenAI client initialization failure.\")\n",
        "\n",
        "\n",
        "# Step 4: Upload Excel file\n",
        "print(\"\\nPlease upload your Excel file (e.g., 'texts_to_embed.xlsx' with a 'text' column):\")\n",
        "# Pastikan ini dijalankan di environment yang mendukung files.upload() seperti Google Colab\n",
        "try:\n",
        "    uploaded = files.upload()\n",
        "except NameError:\n",
        "    print(\"‚ö†Ô∏è `files.upload()` is not available. This suggests you are not running in Google Colab.\")\n",
        "    print(\"   Please provide the file path manually if running locally.\")\n",
        "    # Contoh untuk input manual jika tidak di Colab:\n",
        "    # filename_manual = input(\"Enter the full path to your Excel file: \")\n",
        "    # if os.path.exists(filename_manual):\n",
        "    #     filename = filename_manual\n",
        "    #     uploaded = {filename: None} # Simulasikan struktur 'uploaded'\n",
        "    # else:\n",
        "    #     print(f\"‚ùå File not found at path: {filename_manual}\")\n",
        "    #     uploaded = None\n",
        "    # Untuk demo ini, kita akan hentikan jika tidak di Colab dan tidak ada path manual\n",
        "    sys.exit(\"File upload mechanism not available or handled.\")\n",
        "\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"‚ùå No file uploaded. Please run the cell again and upload a file.\")\n",
        "    sys.exit(\"Exiting because no file was uploaded.\")\n",
        "else:\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    print(f\"\\n‚úÖ File '{filename}' uploaded successfully.\")\n",
        "\n",
        "    # Step 5: Read the Excel file\n",
        "    df = None # Inisialisasi df\n",
        "    try:\n",
        "        df = pd.read_excel(filename)\n",
        "        if 'text' not in df.columns:\n",
        "            print(f\"‚ùå Error: The Excel file '{filename}' must contain a column named 'text'.\")\n",
        "            print(f\"   Found columns: {df.columns.tolist()}\")\n",
        "            sys.exit(\"Exiting due to missing 'text' column.\")\n",
        "        else:\n",
        "            # Bersihkan teks: hapus NaN, konversi ke string, hapus spasi ekstra\n",
        "            texts_from_excel = df['text'].dropna().astype(str).str.strip().tolist()\n",
        "            # Filter string kosong setelah strip\n",
        "            texts_to_embed = [text for text in texts_from_excel if text]\n",
        "\n",
        "            if not texts_to_embed:\n",
        "                print(\"‚ùå No valid, non-empty texts found in the 'text' column after cleaning.\")\n",
        "                sys.exit(\"Exiting as no texts to embed.\")\n",
        "            print(f\"‚úÖ Found {len(texts_to_embed)} non-empty texts in the 'text' column to process.\")\n",
        "\n",
        "            # (Opsional) Jika ada kolom kategori, siapkan juga\n",
        "            categories_for_texts = None\n",
        "            if 'category' in df.columns:\n",
        "                # Ambil kategori yang sesuai dengan teks yang valid\n",
        "                # Ini butuh pencocokan hati-hati jika ada NaN atau baris kosong di 'text'\n",
        "                # Cara aman: buat DataFrame baru hanya dengan baris yang 'text'-nya valid\n",
        "                valid_texts_df = df.dropna(subset=['text'])\n",
        "                valid_texts_df['text'] = valid_texts_df['text'].astype(str).str.strip()\n",
        "                valid_texts_df = valid_texts_df[valid_texts_df['text'] != '']\n",
        "\n",
        "                if len(valid_texts_df) == len(texts_to_embed):\n",
        "                    categories_for_texts = valid_texts_df['category'].astype(str).tolist()\n",
        "                    print(f\"‚úÖ Found {len(categories_for_texts)} corresponding categories.\")\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è Could not perfectly align categories with texts. Categories will not be used for coloring.\")\n",
        "\n",
        "\n",
        "            # Step 6: Generate embeddings and store them\n",
        "            texts_with_embeddings_and_category = [] # Akan menyimpan {'text': original_text, 'embedding': vector, 'category': category_value}\n",
        "\n",
        "            embedding_model = \"text-embedding-3-small\"\n",
        "            print(f\"\\n‚öôÔ∏è Generating embeddings using model: {embedding_model}\")\n",
        "\n",
        "            for i, text_content in enumerate(texts_to_embed):\n",
        "                # Teks sudah dipastikan tidak kosong di sini\n",
        "                try:\n",
        "                    print(f\"   Processing text {i+1}/{len(texts_to_embed)}: \\\"{text_content[:60].replace(chr(10), ' ').replace(chr(13), ' ')}...\\\"\")\n",
        "                    response = client.embeddings.create(\n",
        "                        model=embedding_model,\n",
        "                        input=text_content,\n",
        "                        encoding_format=\"float\"\n",
        "                    )\n",
        "                    embedding = response.data[0].embedding\n",
        "                    item_to_store = {'text': text_content, 'embedding': embedding}\n",
        "\n",
        "                    # Tambahkan kategori jika tersedia dan cocok\n",
        "                    if categories_for_texts and i < len(categories_for_texts):\n",
        "                        item_to_store['category'] = categories_for_texts[i]\n",
        "\n",
        "                    texts_with_embeddings_and_category.append(item_to_store)\n",
        "\n",
        "                    # time.sleep(0.05) # Delay sangat kecil, biasanya tidak perlu untuk model baru\n",
        "                except openai.APIError as e:\n",
        "                    print(f\"   ‚ùå OpenAI API Error for text {i+1} (\\\"{text_content[:30]}...\\\"): {e}\")\n",
        "                    print(f\"      Skipping this text.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"   ‚ùå An unexpected error occurred for text {i+1} (\\\"{text_content[:30]}...\\\"): {e}\")\n",
        "                    print(f\"      Skipping this text.\")\n",
        "\n",
        "            if texts_with_embeddings_and_category:\n",
        "                print(f\"\\n‚úÖ Successfully generated and stored embeddings for {len(texts_with_embeddings_and_category)} texts.\")\n",
        "            else:\n",
        "                print(\"\\n‚ö†Ô∏è No embeddings were generated. Check your Excel file content or API errors.\")\n",
        "                sys.exit(\"Exiting as no embeddings could be generated.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: File '{filename}' not found after upload (should not happen if upload was successful).\")\n",
        "        sys.exit(\"Exiting due to FileNotFoundError.\")\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"‚ùå Error: The Excel file '{filename}' is empty or has no data.\")\n",
        "        sys.exit(\"Exiting due to EmptyDataError.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred while processing the Excel file or generating embeddings: {e}\")\n",
        "        sys.exit(f\"Exiting due to an unexpected error: {e}\")\n",
        "\n",
        "\n",
        "# --- BLOK 2: VISUALISASI T-SNE DENGAN PARAMETER DISESUAIKAN ---\n",
        "\n",
        "if 'texts_with_embeddings_and_category' in locals() and texts_with_embeddings_and_category:\n",
        "    print(f\"\\n--- Starting t-SNE Visualization ---\")\n",
        "    print(f\"‚úÖ Using {len(texts_with_embeddings_and_category)} items with embeddings.\")\n",
        "\n",
        "    # Step 1: Ekstrak embeddings, teks, dan kategori (jika ada)\n",
        "    all_embeddings_list = []\n",
        "    all_texts_list = []\n",
        "    all_categories_list = [] # Untuk pewarnaan\n",
        "    has_categories = False\n",
        "\n",
        "    for item in texts_with_embeddings_and_category:\n",
        "        if 'embedding' in item and 'text' in item:\n",
        "            all_embeddings_list.append(item['embedding'])\n",
        "            all_texts_list.append(item['text'])\n",
        "            if 'category' in item:\n",
        "                all_categories_list.append(item['category'])\n",
        "                has_categories = True # Set flag jika setidaknya satu item punya kategori\n",
        "            elif has_categories: # Jika beberapa punya, beberapa tidak, isi dengan placeholder\n",
        "                 all_categories_list.append(\"N/A\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Skipping item due to missing 'embedding' or 'text': {str(item)[:100]}\")\n",
        "\n",
        "    if not all_embeddings_list or not all_texts_list:\n",
        "        print(\"‚ùå Error: No valid embedding/text data extracted for t-SNE.\")\n",
        "        sys.exit(\"Exiting as no data for t-SNE.\")\n",
        "\n",
        "    embeddings_np = np.array(all_embeddings_list)\n",
        "    print(f\"   Extracted {embeddings_np.shape[0]} embeddings with dimension {embeddings_np.shape[1]}.\")\n",
        "    print(f\"   Extracted {len(all_texts_list)} text labels.\")\n",
        "    if has_categories and len(all_categories_list) == len(all_texts_list):\n",
        "        print(f\"   Extracted {len(all_categories_list)} category labels for coloring.\")\n",
        "    elif has_categories: # Jika jumlah tidak cocok setelah placeholder\n",
        "        print(f\"   ‚ö†Ô∏è Category count mismatch. Will not use categories for coloring.\")\n",
        "        has_categories = False\n",
        "\n",
        "\n",
        "    # Step 2: Reduksi dimensi menggunakan t-SNE dengan parameter disesuaikan\n",
        "    print(\"\\nüîÑ Performing dimensionality reduction with t-SNE (custom parameters)...\")\n",
        "\n",
        "    n_samples = embeddings_np.shape[0]\n",
        "\n",
        "    if n_samples <= 1:\n",
        "        print(f\"‚ùå Error: t-SNE requires at least 2 samples. Found: {n_samples}\")\n",
        "        sys.exit(\"Exiting due to insufficient samples for t-SNE.\")\n",
        "    else:\n",
        "        # --- Parameter t-SNE yang Disesuaikan ---\n",
        "        custom_perplexity = 10.0  # Coba nilai antara 5 dan n_samples-1. Lebih rendah untuk dataset kecil.\n",
        "        if custom_perplexity >= n_samples: # Pastikan perplexity valid\n",
        "            custom_perplexity = max(1.0, float(n_samples - 1) / 2.0)\n",
        "            print(f\"   Adjusted perplexity to {custom_perplexity:.1f} due to small sample size ({n_samples}).\")\n",
        "\n",
        "        custom_n_iter = 2500    # Tingkatkan jumlah iterasi\n",
        "        custom_learning_rate = 'auto' # 'auto' biasanya baik, atau coba nilai eksplisit (misal 100)\n",
        "        # custom_early_exaggeration = 12.0 # Default, bisa juga disesuaikan (misal 15.0)\n",
        "\n",
        "        print(f\"   Using perplexity: {custom_perplexity:.1f}\")\n",
        "        print(f\"   Using n_iter: {custom_n_iter}\")\n",
        "        print(f\"   Using learning_rate: {custom_learning_rate}\")\n",
        "        # print(f\"   Using early_exaggeration: {custom_early_exaggeration}\")\n",
        "\n",
        "        tsne_model = TSNE(\n",
        "            n_components=2,\n",
        "            perplexity=custom_perplexity,\n",
        "            learning_rate=custom_learning_rate,\n",
        "            init='pca', # 'pca' lebih stabil dan seringkali lebih cepat konvergen\n",
        "            n_iter=custom_n_iter,\n",
        "            # early_exaggeration=custom_early_exaggeration,\n",
        "            random_state=42, # Untuk hasil yang konsisten\n",
        "            verbose=0 # Set ke 1 untuk melihat progress jika diinginkan\n",
        "        )\n",
        "        tsne_results = tsne_model.fit_transform(embeddings_np)\n",
        "        print(f\"‚úÖ t-SNE dimensionality reduction complete. Result shape: {tsne_results.shape}\")\n",
        "\n",
        "        # Step 3: Buat DataFrame untuk Plotly\n",
        "        df_tsne = pd.DataFrame({\n",
        "            'tsne_x': tsne_results[:, 0],\n",
        "            'tsne_y': tsne_results[:, 1],\n",
        "            'text_label': all_texts_list\n",
        "        })\n",
        "\n",
        "        color_param_for_plotly = None\n",
        "        if has_categories and len(all_categories_list) == len(df_tsne):\n",
        "            df_tsne['category'] = all_categories_list\n",
        "            color_param_for_plotly = 'category'\n",
        "            print(\"   DataFrame for Plotly includes 'category' column for coloring.\")\n",
        "        else:\n",
        "            print(\"   Categories will not be used for coloring in the plot.\")\n",
        "\n",
        "\n",
        "        print(\"\\nüìä Creating interactive t-SNE visualization with Plotly...\")\n",
        "\n",
        "        # Step 4: Visualisasi Interaktif dengan Plotly\n",
        "        fig = px.scatter(\n",
        "            df_tsne,\n",
        "            x='tsne_x',\n",
        "            y='tsne_y',\n",
        "            hover_name='text_label',\n",
        "            color=color_param_for_plotly, # Akan None jika kategori tidak ada/valid\n",
        "            title=f't-SNE Visualization of Text Embeddings (Perplexity: {custom_perplexity:.0f}, Iter: {custom_n_iter})',\n",
        "            labels={'tsne_x': 't-SNE Component 1', 'tsne_y': 't-SNE Component 2'},\n",
        "            width=1000, # Lebar plot\n",
        "            height=800  # Tinggi plot\n",
        "        )\n",
        "\n",
        "        fig.update_traces(\n",
        "            marker=dict(size=9, line=dict(width=1, color='DarkSlateGrey')), # Sedikit perbesar ukuran titik\n",
        "            selector=dict(mode='markers')\n",
        "        )\n",
        "        fig.update_layout(\n",
        "            hovermode='closest',\n",
        "            legend_title_text='Category' if color_param_for_plotly else ''\n",
        "        )\n",
        "\n",
        "        fig.show()\n",
        "\n",
        "        print(\"\\nüéâ Visualization complete! Hover over points to see their text labels.\")\n",
        "        print(\"   Clusters of points suggest texts with similar embeddings.\")\n",
        "else:\n",
        "    print(\"\\n‚ÑπÔ∏è Skipping t-SNE visualization because 'texts_with_embeddings_and_category' is not available or empty.\")\n",
        "    print(\"   Ensure the Excel processing and embedding generation (Block 1) completed successfully.\")"
      ],
      "metadata": {
        "id": "gM2a9afs36V6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}